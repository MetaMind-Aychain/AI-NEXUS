"""
å¤šAIåä½œç³»ç»Ÿæµ‹è¯•è¿è¡Œå™¨

è¿™ä¸ªè„šæœ¬æä¾›äº†å¤šç§æµ‹è¯•é€‰é¡¹ï¼š
- å¿«é€Ÿæµ‹è¯•ï¼šåŸºæœ¬åŠŸèƒ½éªŒè¯
- å®Œæ•´æµ‹è¯•ï¼šæ‰€æœ‰ç»„ä»¶çš„è¯¦ç»†æµ‹è¯•
- æ€§èƒ½æµ‹è¯•ï¼šç³»ç»Ÿæ€§èƒ½è¯„ä¼°
- æ¼”ç¤ºæ¨¡å¼ï¼šå±•ç¤ºç³»ç»Ÿå·¥ä½œæµç¨‹
"""

import os
import sys
import argparse
import asyncio
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
    print("ğŸ” æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    if sys.version_info < (3, 8):
        print("âŒ éœ€è¦Python 3.8æˆ–æ›´é«˜ç‰ˆæœ¬")
        return False
    
    print(f"âœ… Pythonç‰ˆæœ¬: {sys.version}")
    
    # æ£€æŸ¥å¿…è¦çš„æ¨¡å—
    required_modules = [
        'sqlite3', 'json', 'datetime', 'pathlib', 'tempfile', 'unittest'
    ]
    
    missing_modules = []
    for module in required_modules:
        try:
            __import__(module)
        except ImportError:
            missing_modules.append(module)
    
    if missing_modules:
        print(f"âŒ ç¼ºå°‘å¿…è¦æ¨¡å—: {', '.join(missing_modules)}")
        return False
    
    print("âœ… æ‰€æœ‰å¿…è¦æ¨¡å—å·²å®‰è£…")
    
    # æ£€æŸ¥OpenAI APIå¯†é’¥
    has_openai_key = bool(os.getenv('OPENAI_API_KEY'))
    if has_openai_key:
        print("âœ… æ£€æµ‹åˆ°OPENAI_API_KEY")
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°OPENAI_API_KEYï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    
    return True

def run_quick_test():
    """å¿«é€Ÿæµ‹è¯• - éªŒè¯åŸºæœ¬åŠŸèƒ½"""
    print("\nğŸƒâ€â™‚ï¸ æ‰§è¡Œå¿«é€Ÿæµ‹è¯•...")
    print("=" * 40)
    
    try:
        from test_multi_ai_system import (
            TestSharedMemorySystem, 
            TestSupervisorAI,
            run_mock_workflow_test
        )
        import unittest
        
        # è¿è¡Œæ ¸å¿ƒç»„ä»¶æµ‹è¯•
        test_suite = unittest.TestSuite()
        test_suite.addTest(unittest.makeSuite(TestSharedMemorySystem))
        test_suite.addTest(unittest.makeSuite(TestSupervisorAI))
        
        runner = unittest.TextTestRunner(verbosity=1)
        result = runner.run(test_suite)
        
        # è¿è¡Œæ¨¡æ‹Ÿå·¥ä½œæµç¨‹
        run_mock_workflow_test()
        
        if result.failures or result.errors:
            print("âŒ å¿«é€Ÿæµ‹è¯•å‘ç°é—®é¢˜")
            return False
        else:
            print("âœ… å¿«é€Ÿæµ‹è¯•é€šè¿‡ï¼")
            return True
            
    except Exception as e:
        print(f"âŒ å¿«é€Ÿæµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return False

def run_full_test():
    """å®Œæ•´æµ‹è¯• - æ‰€æœ‰ç»„ä»¶è¯¦ç»†æµ‹è¯•"""
    print("\nğŸ”¬ æ‰§è¡Œå®Œæ•´æµ‹è¯•...")
    print("=" * 40)
    
    try:
        from test_multi_ai_system import main as run_all_tests
        return run_all_tests()
        
    except Exception as e:
        print(f"âŒ å®Œæ•´æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return False

def run_performance_test():
    """æ€§èƒ½æµ‹è¯•"""
    print("\nâš¡ æ‰§è¡Œæ€§èƒ½æµ‹è¯•...")
    print("=" * 40)
    
    try:
        from test_multi_ai_system import run_performance_test
        run_performance_test()
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return False

async def run_demo():
    """æ¼”ç¤ºæ¨¡å¼ - å±•ç¤ºç³»ç»Ÿå·¥ä½œæµç¨‹"""
    print("\nğŸ­ æ¼”ç¤ºæ¨¡å¼...")
    print("=" * 40)
    
    try:
        # å¯¼å…¥å¿…è¦æ¨¡å—
        from multi_ai_system import create_orchestrator
        from test_multi_ai_system import MockAI
        import tempfile
        
        # åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•
        temp_dir = tempfile.mkdtemp()
        print(f"ğŸ“ å·¥ä½œç›®å½•: {temp_dir}")
        
        # åˆ›å»ºç¼–æ’å™¨
        orchestrator = create_orchestrator(
            work_dir=temp_dir,
            ai_config={'model_name': 'mock-gpt-4o'}
        )
        
        # ä½¿ç”¨æ¨¡æ‹ŸAI
        orchestrator.main_ai = MockAI()
        orchestrator._init_ai_components()
        
        print("ğŸ¤– å¤šAIç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        
        # æ¼”ç¤ºéœ€æ±‚
        requirement = """
å¼€å‘ä¸€ä¸ªç®€å•çš„ä»»åŠ¡ç®¡ç†ç³»ç»Ÿï¼ŒåŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š
1. æ·»åŠ ä»»åŠ¡
2. æŸ¥çœ‹ä»»åŠ¡åˆ—è¡¨
3. æ ‡è®°ä»»åŠ¡å®Œæˆ
4. åˆ é™¤ä»»åŠ¡
5. ç®€å•çš„å‘½ä»¤è¡Œç•Œé¢
        """
        
        print(f"ğŸ“ æ¼”ç¤ºéœ€æ±‚:\n{requirement}")
        print("\nğŸ”„ å¼€å§‹å·¥ä½œæµç¨‹æ¼”ç¤º...")
        
        # è·å–ä¼šè¯çŠ¶æ€
        status = orchestrator.get_session_status()
        print(f"ğŸ“Š å½“å‰çŠ¶æ€: {status['status']}")
        print(f"ğŸ¯ å½“å‰é˜¶æ®µ: {status['current_stage']}")
        
        # æ¼”ç¤ºå„ä¸ªAIç»„ä»¶
        print("\n1ï¸âƒ£ ç›‘ç®¡AIè´¨é‡åˆ†ææ¼”ç¤º...")
        from gpt_engineer.core.files_dict import FilesDict
        
        sample_code = FilesDict({
            'task_manager.py': '''
def add_task(tasks, task):
    """æ·»åŠ æ–°ä»»åŠ¡"""
    tasks.append({"id": len(tasks) + 1, "text": task, "done": False})
    return tasks

def list_tasks(tasks):
    """æ˜¾ç¤ºä»»åŠ¡åˆ—è¡¨"""
    for task in tasks:
        status = "âœ“" if task["done"] else "â—‹"
        print(f"{status} {task['id']}. {task['text']}")

def main():
    tasks = []
    while True:
        print("\\nä»»åŠ¡ç®¡ç†å™¨")
        print("1. æ·»åŠ ä»»åŠ¡")
        print("2. æŸ¥çœ‹ä»»åŠ¡")
        print("3. é€€å‡º")
        
        choice = input("è¯·é€‰æ‹©: ")
        if choice == "1":
            task = input("è¾“å…¥ä»»åŠ¡: ")
            add_task(tasks, task)
        elif choice == "2":
            list_tasks(tasks)
        elif choice == "3":
            break

if __name__ == "__main__":
    main()
'''
        })
        
        quality_report = orchestrator.supervisor_ai.analyze_quality(sample_code)
        print(f"   ğŸ“Š ä»£ç è´¨é‡è¯„åˆ†: {quality_report.overall_score:.1f}/100")
        print(f"   ğŸ” å‘ç°é—®é¢˜: {len(quality_report.issues)} ä¸ª")
        print(f"   ğŸ’¡ æ”¹è¿›å»ºè®®: {len(quality_report.suggestions)} æ¡")
        
        print("\n2ï¸âƒ£ æµ‹è¯•AIæµ‹è¯•ç”Ÿæˆæ¼”ç¤º...")
        requirements = {'description': 'ä»»åŠ¡ç®¡ç†ç³»ç»Ÿ', 'features': ['æ·»åŠ ä»»åŠ¡', 'æŸ¥çœ‹ä»»åŠ¡']}
        test_files = orchestrator.test_ai.generate_tests(sample_code, requirements)
        print(f"   ğŸ§ª ç”Ÿæˆæµ‹è¯•æ–‡ä»¶: {len(test_files)} ä¸ª")
        for filename in test_files.keys():
            print(f"      - {filename}")
        
        print("\n3ï¸âƒ£ éƒ¨ç½²AIé…ç½®ç”Ÿæˆæ¼”ç¤º...")
        deploy_config = orchestrator.deploy_ai.generate_deployment_config(sample_code)
        print(f"   ğŸš€ é¡¹ç›®ç±»å‹: {deploy_config['project_type']}")
        print(f"   ğŸ“¦ å…¥å£æ–‡ä»¶: {deploy_config['entrypoint']}")
        print(f"   ğŸ”Œ æ£€æµ‹ç«¯å£: {deploy_config['ports']}")
        
        print("\n4ï¸âƒ£ å…±äº«è®°å¿†ç³»ç»Ÿæ¼”ç¤º...")
        stats = orchestrator.shared_memory.get_statistics()
        print(f"   ğŸ’¾ å­˜å‚¨äº‹ä»¶: {stats['events']['total']} ä¸ª")
        print(f"   ğŸ“š çŸ¥è¯†æ¡ç›®: {stats['knowledge']['total']} ä¸ª")
        
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("ğŸ’¡ è¿™åªæ˜¯ç³»ç»ŸåŠŸèƒ½çš„ä¸€ä¸ªç®€å•å±•ç¤º")
        print("ğŸ’¡ å®Œæ•´çš„å·¥ä½œæµç¨‹ä¼šè‡ªåŠ¨æ‰§è¡Œæ‰€æœ‰è¿™äº›æ­¥éª¤")
        
        # æ¸…ç†
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šAIåä½œç³»ç»Ÿæµ‹è¯•è¿è¡Œå™¨')
    parser.add_argument('--mode', '-m', 
                       choices=['quick', 'full', 'performance', 'demo', 'check'],
                       default='quick',
                       help='æµ‹è¯•æ¨¡å¼ (é»˜è®¤: quick)')
    
    parser.add_argument('--verbose', '-v', 
                       action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¤šAIåä½œç³»ç»Ÿæµ‹è¯•è¿è¡Œå™¨")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        print("\nâŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·è§£å†³ä¸Šè¿°é—®é¢˜åé‡è¯•")
        return False
    
    success = False
    
    if args.mode == 'check':
        print("\nâœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆï¼Œç³»ç»Ÿå¯ä»¥è¿è¡Œ")
        success = True
        
    elif args.mode == 'quick':
        success = run_quick_test()
        
    elif args.mode == 'full':
        success = run_full_test()
        
    elif args.mode == 'performance':
        success = run_performance_test()
        
    elif args.mode == 'demo':
        success = asyncio.run(run_demo())
    
    print("\n" + "=" * 50)
    
    if success:
        print("ğŸ‰ æµ‹è¯•æ‰§è¡ŒæˆåŠŸï¼")
        print("\nğŸ“‹ åç»­å»ºè®®:")
        print("   1. æŸ¥çœ‹ README_å¤šAIåä½œç³»ç»Ÿ.md äº†è§£è¯¦ç»†ç”¨æ³•")
        print("   2. è¿è¡Œ python -m multi_ai_system.examples.usage_example")
        print("   3. è®¾ç½® OPENAI_API_KEY ä½“éªŒå®Œæ•´åŠŸèƒ½")
    else:
        print("âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)