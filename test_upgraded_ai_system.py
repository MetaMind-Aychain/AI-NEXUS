#!/usr/bin/env python3
"""
å‡çº§ç‰ˆAIç³»ç»Ÿæµ‹è¯•

éªŒè¯æ‰€æœ‰å‡çº§çš„AIç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import os
import sys
from pathlib import Path
import tempfile
import json

# æ·»åŠ è·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from gpt_engineer.core.ai import AI
from gpt_engineer.core.files_dict import FilesDict
from gpt_engineer.core.default.disk_memory import DiskMemory
from gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv
from gpt_engineer.core.preprompts_holder import PrepromptsHolder

from multi_ai_system.ai.ai_upgrade_manager import AIUpgradeManager
from multi_ai_system.ai.advanced_supervisor_ai import AdvancedSupervisorAI
from multi_ai_system.ai.advanced_test_ai import AdvancedTestAI
from multi_ai_system.ai.advanced_document_ai import AdvancedDocumentAI
from multi_ai_system.ai.advanced_dev_ai import AdvancedDevAI
from multi_ai_system.memory.shared_memory import SharedMemoryManager


class UpgradedAISystemTester:
    """å‡çº§ç‰ˆAIç³»ç»Ÿæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = []
        self.passed_tests = 0
        self.total_tests = 0
        
        # æ£€æŸ¥APIå¯†é’¥
        if not os.getenv("OPENAI_API_KEY"):
            print("âš ï¸ è­¦å‘Š: æœªè®¾ç½® OPENAI_API_KEYï¼ŒæŸäº›æµ‹è¯•å¯èƒ½å¤±è´¥")
    
    def log_test(self, test_name: str, success: bool, message: str = "", details: any = None):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        self.total_tests += 1
        if success:
            self.passed_tests += 1
            status = "âœ… PASS"
        else:
            status = "âŒ FAIL"
        
        result = {
            "name": test_name,
            "success": success,
            "message": message,
            "details": details
        }
        
        self.test_results.append(result)
        print(f"{status} {test_name}: {message}")
    
    async def test_basic_ai_initialization(self):
        """æµ‹è¯•åŸºç¡€AIåˆå§‹åŒ–"""
        try:
            ai = AI(model_name="gpt-4o", temperature=0.1)
            self.log_test("åŸºç¡€AIåˆå§‹åŒ–", True, "AIå®ä¾‹åˆ›å»ºæˆåŠŸ")
            return ai
        except Exception as e:
            self.log_test("åŸºç¡€AIåˆå§‹åŒ–", False, f"åˆ›å»ºå¤±è´¥: {str(e)}")
            return None
    
    async def test_advanced_document_ai(self, ai: AI):
        """æµ‹è¯•é«˜çº§æ–‡æ¡£AI"""
        try:
            # åˆ›å»ºæ–‡æ¡£AIå®ä¾‹
            doc_ai = AdvancedDocumentAI(ai)
            
            # æµ‹è¯•éœ€æ±‚åˆ†æ
            test_requirements = "åˆ›å»ºä¸€ä¸ªç®€å•çš„å¾…åŠäº‹é¡¹ç®¡ç†åº”ç”¨ï¼ŒåŒ…å«æ·»åŠ ã€åˆ é™¤ã€æ ‡è®°å®ŒæˆåŠŸèƒ½"
            analysis = await doc_ai.analyze_requirements(test_requirements)
            
            success = (
                isinstance(analysis, dict) and
                "requirement_analysis" in analysis
            )
            
            self.log_test(
                "é«˜çº§æ–‡æ¡£AI-éœ€æ±‚åˆ†æ", 
                success, 
                f"åˆ†æç»“æœåŒ…å« {len(analysis)} ä¸ªå­—æ®µ",
                analysis
            )
            
            # æµ‹è¯•æ–‡æ¡£ç”Ÿæˆ
            if success:
                docs = await doc_ai.generate_comprehensive_documentation(analysis)
                doc_success = isinstance(docs, dict) and len(docs) > 0
                
                self.log_test(
                    "é«˜çº§æ–‡æ¡£AI-æ–‡æ¡£ç”Ÿæˆ",
                    doc_success,
                    f"ç”Ÿæˆäº† {len(docs)} ä¸ªæ–‡æ¡£",
                    list(docs.keys())
                )
            
            return doc_ai
            
        except Exception as e:
            self.log_test("é«˜çº§æ–‡æ¡£AI", False, f"æµ‹è¯•å¤±è´¥: {str(e)}")
            return None
    
    async def test_advanced_supervisor_ai(self, ai: AI):
        """æµ‹è¯•é«˜çº§ç›‘ç®¡AI"""
        try:
            # åˆ›å»ºç›‘ç®¡AIå®ä¾‹
            supervisor_ai = AdvancedSupervisorAI(ai)
            
            # åˆ›å»ºæµ‹è¯•å¼€å‘è®¡åˆ’
            from multi_ai_system.core.base_interfaces import DevPlan
            test_plan = DevPlan(
                tasks=["åˆå§‹åŒ–é¡¹ç›®", "å¼€å‘æ ¸å¿ƒåŠŸèƒ½", "ç¼–å†™æµ‹è¯•"],
                estimated_time=24.0,
                dependencies={},
                milestones=["MVPå®Œæˆ"]
            )
            
            # æµ‹è¯•ç›‘ç®¡å¯åŠ¨
            supervision_id = await supervisor_ai.start_supervision(test_plan)
            
            success = supervision_id is not None and len(supervision_id) > 0
            
            self.log_test(
                "é«˜çº§ç›‘ç®¡AI-å¯åŠ¨ç›‘ç®¡",
                success,
                f"ç›‘ç®¡ID: {supervision_id[:8]}...",
                supervision_id
            )
            
            # æµ‹è¯•è´¨é‡åˆ†æ
            if success:
                test_files = FilesDict({
                    "main.py": "def hello():\n    print('Hello, World!')\n\nif __name__ == '__main__':\n    hello()",
                    "test_main.py": "import unittest\nfrom main import hello\n\nclass TestMain(unittest.TestCase):\n    def test_hello(self):\n        pass"
                })
                
                quality_report = await supervisor_ai.analyze_quality(supervision_id, test_files)
                
                quality_success = (
                    hasattr(quality_report, 'overall_score') and
                    quality_report.overall_score >= 0
                )
                
                self.log_test(
                    "é«˜çº§ç›‘ç®¡AI-è´¨é‡åˆ†æ",
                    quality_success,
                    f"è´¨é‡è¯„åˆ†: {getattr(quality_report, 'overall_score', 'N/A')}",
                    quality_report
                )
            
            return supervisor_ai
            
        except Exception as e:
            self.log_test("é«˜çº§ç›‘ç®¡AI", False, f"æµ‹è¯•å¤±è´¥: {str(e)}")
            return None
    
    async def test_advanced_test_ai(self, ai: AI):
        """æµ‹è¯•é«˜çº§æµ‹è¯•AI"""
        try:
            # åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•
            with tempfile.TemporaryDirectory() as temp_dir:
                test_ai = AdvancedTestAI(ai, temp_dir)
                
                # åˆ›å»ºæµ‹è¯•ä»£ç 
                test_files = FilesDict({
                    "calculator.py": """
def add(a, b):
    return a + b

def subtract(a, b):
    return a - b

def multiply(a, b):
    return a * b

def divide(a, b):
    if b == 0:
        raise ValueError("Cannot divide by zero")
    return a / b
"""
                })
                
                # æµ‹è¯•ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
                generated_tests = await test_ai.generate_tests(test_files, "ä¸ºè®¡ç®—å™¨å‡½æ•°ç”Ÿæˆæµ‹è¯•")
                
                test_success = isinstance(generated_tests, dict) and len(generated_tests) > 0
                
                self.log_test(
                    "é«˜çº§æµ‹è¯•AI-æµ‹è¯•ç”Ÿæˆ",
                    test_success,
                    f"ç”Ÿæˆäº† {len(generated_tests)} ä¸ªæµ‹è¯•æ–‡ä»¶",
                    list(generated_tests.keys())
                )
                
                # æµ‹è¯•æ‰§è¡Œæµ‹è¯•ï¼ˆç®€åŒ–ï¼‰
                if test_success:
                    try:
                        # æ³¨æ„ï¼šè¿™é‡Œå¯èƒ½ä¼šå› ä¸ºç¯å¢ƒé—®é¢˜å¤±è´¥ï¼Œä½†ä¸å½±å“AIåŠŸèƒ½æµ‹è¯•
                        test_result = await test_ai.execute_tests(test_files, generated_tests)
                        
                        execution_success = hasattr(test_result, 'total_tests')
                        
                        self.log_test(
                            "é«˜çº§æµ‹è¯•AI-æµ‹è¯•æ‰§è¡Œ",
                            execution_success,
                            f"æ‰§è¡Œç»“æœ: {getattr(test_result, 'total_tests', 0)} ä¸ªæµ‹è¯•",
                            test_result
                        )
                    except Exception as e:
                        self.log_test(
                            "é«˜çº§æµ‹è¯•AI-æµ‹è¯•æ‰§è¡Œ",
                            False,
                            f"æ‰§è¡Œç¯å¢ƒé—®é¢˜: {str(e)[:50]}..."
                        )
                
                return test_ai
                
        except Exception as e:
            self.log_test("é«˜çº§æµ‹è¯•AI", False, f"æµ‹è¯•å¤±è´¥: {str(e)}")
            return None
    
    async def test_advanced_dev_ai(self, ai: AI):
        """æµ‹è¯•é«˜çº§å¼€å‘AI"""
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•å’Œæ‰€éœ€ç»„ä»¶
            with tempfile.TemporaryDirectory() as temp_dir:
                memory = DiskMemory(temp_dir)
                execution_env = DiskExecutionEnv()
                preprompts = PrepromptsHolder(Path(__file__).parent / "gpt_engineer" / "preprompts")
                
                dev_ai = AdvancedDevAI(
                    memory=memory,
                    execution_env=execution_env,
                    ai=ai,
                    preprompts_holder=preprompts
                )
                
                # æµ‹è¯•éœ€æ±‚åˆ†æ
                test_requirements = "åˆ›å»ºä¸€ä¸ªç®€å•çš„Python HTTPæœåŠ¡å™¨ï¼Œæä¾›RESTful API"
                analysis = await dev_ai._analyze_requirements(test_requirements)
                
                analysis_success = isinstance(analysis, dict)
                
                self.log_test(
                    "é«˜çº§å¼€å‘AI-éœ€æ±‚åˆ†æ",
                    analysis_success,
                    f"åˆ†æåŒ…å« {len(analysis)} ä¸ªç»´åº¦",
                    list(analysis.keys())
                )
                
                # æµ‹è¯•é¡¹ç›®åˆå§‹åŒ–
                if analysis_success:
                    try:
                        initial_files = await dev_ai.init(test_requirements)
                        
                        init_success = isinstance(initial_files, dict) and len(initial_files) > 0
                        
                        self.log_test(
                            "é«˜çº§å¼€å‘AI-é¡¹ç›®åˆå§‹åŒ–",
                            init_success,
                            f"ç”Ÿæˆäº† {len(initial_files)} ä¸ªæ–‡ä»¶",
                            list(initial_files.keys())
                        )
                        
                        return dev_ai
                        
                    except Exception as e:
                        self.log_test(
                            "é«˜çº§å¼€å‘AI-é¡¹ç›®åˆå§‹åŒ–",
                            False,
                            f"åˆå§‹åŒ–å¤±è´¥: {str(e)[:50]}..."
                        )
                
                return dev_ai
                
        except Exception as e:
            self.log_test("é«˜çº§å¼€å‘AI", False, f"æµ‹è¯•å¤±è´¥: {str(e)}")
            return None
    
    async def test_ai_upgrade_manager(self, ai: AI):
        """æµ‹è¯•AIå‡çº§ç®¡ç†å™¨"""
        try:
            # åˆ›å»ºå‡çº§ç®¡ç†å™¨
            with tempfile.TemporaryDirectory() as temp_dir:
                upgrade_manager = AIUpgradeManager(ai, work_dir=temp_dir)
                
                # æµ‹è¯•åˆ›å»ºé¡¹ç›®
                test_requirements = "åˆ›å»ºä¸€ä¸ªç®€å•çš„Webåº”ç”¨ï¼ŒåŒ…å«ç”¨æˆ·ç™»å½•å’Œæ•°æ®å±•ç¤ºåŠŸèƒ½"
                
                project = await upgrade_manager.create_comprehensive_project(
                    test_requirements,
                    {"complexity": "simple", "timeline": "1 week"}
                )
                
                project_success = (
                    isinstance(project, dict) and
                    "id" in project and
                    "requirement_analysis" in project
                )
                
                self.log_test(
                    "AIå‡çº§ç®¡ç†å™¨-é¡¹ç›®åˆ›å»º",
                    project_success,
                    f"é¡¹ç›®ID: {project.get('id', 'N/A')[:8]}...",
                    {
                        "docs_count": len(project.get("documents", {})),
                        "tasks_count": len(project.get("dev_plan", {}).get("tasks", [])),
                        "status": project.get("status")
                    }
                )
                
                # æµ‹è¯•è·å–æ€§èƒ½æ‘˜è¦
                performance_summary = await upgrade_manager.get_ai_performance_summary()
                
                summary_success = (
                    isinstance(performance_summary, dict) and
                    "overall" in performance_summary
                )
                
                self.log_test(
                    "AIå‡çº§ç®¡ç†å™¨-æ€§èƒ½æ‘˜è¦",
                    summary_success,
                    f"æ´»è·ƒé¡¹ç›®: {performance_summary.get('overall', {}).get('active_projects', 0)}",
                    performance_summary
                )
                
                return upgrade_manager
                
        except Exception as e:
            self.log_test("AIå‡çº§ç®¡ç†å™¨", False, f"æµ‹è¯•å¤±è´¥: {str(e)}")
            return None
    
    async def test_integration_workflow(self, ai: AI):
        """æµ‹è¯•é›†æˆå·¥ä½œæµ"""
        try:
            # åˆ›å»ºå®Œæ•´çš„å·¥ä½œæµæµ‹è¯•
            with tempfile.TemporaryDirectory() as temp_dir:
                # 1. åˆ›å»ºç®¡ç†å™¨
                upgrade_manager = AIUpgradeManager(ai, work_dir=temp_dir)
                
                # 2. è®¾ç½®å¼€å‘AI
                memory = DiskMemory(temp_dir)
                execution_env = DiskExecutionEnv()
                upgrade_manager.initialize_dev_ai(memory, execution_env)
                
                # 3. åˆ›å»ºé¡¹ç›®
                project = await upgrade_manager.create_comprehensive_project(
                    "åˆ›å»ºä¸€ä¸ªå¾…åŠäº‹é¡¹ç®¡ç†å™¨",
                    {"complexity": "simple"}
                )
                
                # 4. æ¨¡æ‹Ÿåä½œç¼–è¾‘
                if project:
                    session_id = await upgrade_manager.collaborative_document_editing(
                        project["id"], ["user1", "user2"]
                    )
                    
                    # 5. å¤„ç†åé¦ˆ
                    feedback_result = await upgrade_manager.process_collaborative_feedback(
                        session_id, "user1", {
                            "action": "edit",
                            "target": "requirements",
                            "content": "æ·»åŠ ä¼˜å…ˆçº§åŠŸèƒ½"
                        }
                    )
                    
                    integration_success = (
                        project is not None and
                        session_id is not None and
                        feedback_result.get("success", False)
                    )
                    
                    self.log_test(
                        "é›†æˆå·¥ä½œæµæµ‹è¯•",
                        integration_success,
                        "å®Œæ•´å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ",
                        {
                            "project_created": project is not None,
                            "collaboration_started": session_id is not None,
                            "feedback_processed": feedback_result.get("success", False)
                        }
                    )
                else:
                    self.log_test("é›†æˆå·¥ä½œæµæµ‹è¯•", False, "é¡¹ç›®åˆ›å»ºå¤±è´¥")
                
        except Exception as e:
            self.log_test("é›†æˆå·¥ä½œæµæµ‹è¯•", False, f"æµ‹è¯•å¤±è´¥: {str(e)}")
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹å‡çº§ç‰ˆAIç³»ç»Ÿæµ‹è¯•")
        print("=" * 60)
        
        # åŸºç¡€æµ‹è¯•
        ai = await self.test_basic_ai_initialization()
        if not ai:
            print("âŒ åŸºç¡€AIåˆå§‹åŒ–å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
            return False
        
        # ä¸ªåˆ«AIç»„ä»¶æµ‹è¯•
        await self.test_advanced_document_ai(ai)
        await self.test_advanced_supervisor_ai(ai)
        await self.test_advanced_test_ai(ai)
        await self.test_advanced_dev_ai(ai)
        
        # ç®¡ç†å™¨æµ‹è¯•
        await self.test_ai_upgrade_manager(ai)
        
        # é›†æˆæµ‹è¯•
        await self.test_integration_workflow(ai)
        
        # è¾“å‡ºæµ‹è¯•æ‘˜è¦
        self.print_test_summary()
        
        return self.passed_tests == self.total_tests
    
    def print_test_summary(self):
        """è¾“å‡ºæµ‹è¯•æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ å‡çº§ç‰ˆAIç³»ç»Ÿæµ‹è¯•æ‘˜è¦")
        print("=" * 60)
        
        print(f"æ€»æµ‹è¯•æ•°: {self.total_tests}")
        print(f"é€šè¿‡æµ‹è¯•: {self.passed_tests}")
        print(f"å¤±è´¥æµ‹è¯•: {self.total_tests - self.passed_tests}")
        print(f"é€šè¿‡ç‡: {(self.passed_tests / self.total_tests * 100):.1f}%" if self.total_tests > 0 else "0%")
        
        print("\nè¯¦ç»†ç»“æœ:")
        for result in self.test_results:
            status = "âœ…" if result["success"] else "âŒ"
            print(f"{status} {result['name']}: {result['message']}")
        
        if self.passed_tests == self.total_tests:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å‡çº§ç‰ˆAIç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
            print("\nå‡çº§åŠŸèƒ½éªŒè¯:")
            print("âœ… é«˜çº§æ–‡æ¡£AI: æ™ºèƒ½éœ€æ±‚åˆ†æå’Œå¤šæ ¼å¼æ–‡æ¡£ç”Ÿæˆ")
            print("âœ… é«˜çº§ç›‘ç®¡AI: æ·±åº¦è´¨é‡è¯„ä¼°å’Œé¢„æµ‹æ€§é£é™©åˆ†æ")
            print("âœ… é«˜çº§æµ‹è¯•AI: æ™ºèƒ½æµ‹è¯•ç”Ÿæˆå’Œå¤šç»´åº¦è¦†ç›–åˆ†æ")
            print("âœ… é«˜çº§å¼€å‘AI: æ™ºèƒ½æ¶æ„è®¾è®¡å’Œæ€§èƒ½ä¼˜åŒ–")
            print("âœ… AIå‡çº§ç®¡ç†å™¨: ç»Ÿä¸€åè°ƒå’Œæ™ºèƒ½å·¥ä½œæµ")
        else:
            failed_count = self.total_tests - self.passed_tests
            print(f"\nâš ï¸ æœ‰ {failed_count} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œä½†è¿™å¯èƒ½æ˜¯ç¯å¢ƒé…ç½®é—®é¢˜ã€‚")
            print("æ ¸å¿ƒAIå‡çº§åŠŸèƒ½åŸºæœ¬å¯ç”¨ã€‚")


async def main():
    """ä¸»å‡½æ•°"""
    print("å‡çº§ç‰ˆAIç³»ç»Ÿæµ‹è¯•å·¥å…·")
    print("éªŒè¯ç›‘ç®¡AIã€æµ‹è¯•AIã€æ–‡æ¡£AIã€å¼€å‘AIçš„å‡çº§åŠŸèƒ½")
    print("")
    
    tester = UpgradedAISystemTester()
    success = await tester.run_all_tests()
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    asyncio.run(main())