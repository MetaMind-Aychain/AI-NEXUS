"""
é«˜çº§æµ‹è¯•AIå®ç° - å‡çº§ç‰ˆ

æ–°å¢åŠŸèƒ½ï¼š
- æ™ºèƒ½æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ
- å¤šç»´åº¦æµ‹è¯•è¦†ç›–
- è‡ªåŠ¨åŒ–æ€§èƒ½æµ‹è¯•
- ç¼ºé™·æ¨¡å¼è¯†åˆ«
- æµ‹è¯•ä¼˜åŒ–å»ºè®®
- é¢„æµ‹æ€§æµ‹è¯•åˆ†æ
"""

import ast
import json
import os
import re
import subprocess
import tempfile
import uuid
import asyncio
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, AsyncIterator
from dataclasses import dataclass
from enum import Enum

from gpt_engineer.core.ai import AI
from gpt_engineer.core.files_dict import FilesDict

from ..core.base_interfaces import BaseTestAI, TestResult, DeployResult


class TestType(Enum):
    """æµ‹è¯•ç±»å‹"""
    UNIT = "unit"
    INTEGRATION = "integration"
    SYSTEM = "system"
    PERFORMANCE = "performance"
    SECURITY = "security"
    USABILITY = "usability"
    REGRESSION = "regression"
    SMOKE = "smoke"
    LOAD = "load"
    STRESS = "stress"


class TestStrategy(Enum):
    """æµ‹è¯•ç­–ç•¥"""
    COMPREHENSIVE = "comprehensive"  # å…¨é¢æµ‹è¯•
    FOCUSED = "focused"              # èšç„¦æµ‹è¯•
    RISK_BASED = "risk_based"        # åŸºäºé£é™©çš„æµ‹è¯•
    EXPLORATORY = "exploratory"      # æ¢ç´¢æ€§æµ‹è¯•
    ADAPTIVE = "adaptive"            # è‡ªé€‚åº”æµ‹è¯•


@dataclass
class TestCoverage:
    """æµ‹è¯•è¦†ç›–ç‡è¯¦æƒ…"""
    line_coverage: float = 0.0
    branch_coverage: float = 0.0
    function_coverage: float = 0.0
    statement_coverage: float = 0.0
    condition_coverage: float = 0.0
    path_coverage: float = 0.0
    overall_coverage: float = 0.0


@dataclass
class TestMetrics:
    """æµ‹è¯•æŒ‡æ ‡"""
    total_tests: int = 0
    passed_tests: int = 0
    failed_tests: int = 0
    skipped_tests: int = 0
    execution_time: float = 0.0
    coverage: TestCoverage = None
    performance_metrics: Dict[str, float] = None
    quality_score: float = 0.0


@dataclass
class DefectPattern:
    """ç¼ºé™·æ¨¡å¼"""
    pattern_type: str
    description: str
    frequency: int
    severity: str
    affected_components: List[str]
    root_causes: List[str]
    prevention_strategies: List[str]


@dataclass
class TestOptimization:
    """æµ‹è¯•ä¼˜åŒ–å»ºè®®"""
    optimization_type: str
    description: str
    expected_improvement: float
    implementation_effort: str
    priority: int


class AdvancedTestAI(BaseTestAI):
    """
    é«˜çº§æµ‹è¯•AI - å‡çº§ç‰ˆ
    
    æ ¸å¿ƒå‡çº§ï¼š
    1. æ™ºèƒ½æµ‹è¯•ç”Ÿæˆç®—æ³•
    2. å¤šç»´åº¦è¦†ç›–åˆ†æ
    3. æ€§èƒ½å’Œå®‰å…¨æµ‹è¯•
    4. ç¼ºé™·æ¨¡å¼å­¦ä¹ 
    5. é¢„æµ‹æ€§æµ‹è¯•åˆ†æ
    6. è‡ªåŠ¨åŒ–æµ‹è¯•ä¼˜åŒ–
    """
    
    def __init__(self, ai: AI, work_dir: Optional[str] = None):
        self.ai = ai
        self.work_dir = Path(work_dir) if work_dir else Path(tempfile.mkdtemp())
        self.work_dir.mkdir(exist_ok=True)
        
        # å‡çº§çš„æ ¸å¿ƒç»„ä»¶
        self.test_generator = IntelligentTestGenerator(ai)
        self.coverage_analyzer = CoverageAnalyzer(ai)
        self.performance_tester = PerformanceTester(ai)
        self.security_tester = SecurityTester(ai)
        self.defect_analyzer = DefectAnalyzer(ai)
        self.optimization_engine = TestOptimizationEngine(ai)
        
        # æµ‹è¯•æ¡†æ¶é…ç½® - æ‰©å±•ç‰ˆ
        self.test_frameworks = {
            'python': {
                'unit': ['pytest', 'unittest', 'nose2'],
                'integration': ['pytest', 'behave'],
                'performance': ['pytest-benchmark', 'locust'],
                'security': ['bandit', 'safety']
            },
            'javascript': {
                'unit': ['jest', 'mocha', 'jasmine'],
                'integration': ['cypress', 'playwright'],
                'performance': ['lighthouse', 'k6'],
                'security': ['eslint-plugin-security', 'audit']
            },
            'java': {
                'unit': ['junit', 'testng'],
                'integration': ['spring-test', 'testcontainers'],
                'performance': ['jmeter', 'gatling'],
                'security': ['spotbugs', 'owasp-dependency-check']
            }
        }
        
        # æµ‹è¯•ç­–ç•¥é…ç½®
        self.test_strategy = TestStrategy.ADAPTIVE
        self.coverage_targets = {
            'line': 0.90,
            'branch': 0.85,
            'function': 0.95,
            'statement': 0.90
        }
        
        # å­¦ä¹ æ•°æ®
        self.test_history: List[Dict] = []
        self.defect_patterns: List[DefectPattern] = []
        self.optimization_suggestions: List[TestOptimization] = []
    
    async def generate_tests_async(self, files: FilesDict, requirements: Optional[str] = None) -> FilesDict:
        """æ™ºèƒ½æµ‹è¯•ç”Ÿæˆ - å‡çº§ç‰ˆï¼ˆå¼‚æ­¥ï¼‰"""
        print("ğŸ§ª å¯åŠ¨æ™ºèƒ½æµ‹è¯•ç”Ÿæˆ...")
        
        # åˆ†æä»£ç ç»“æ„
        code_analysis = await self._analyze_code_structure(files)
        
        # è¯†åˆ«æµ‹è¯•éœ€æ±‚
        test_requirements = await self._identify_test_requirements(
            files, requirements, code_analysis
        )
        
        # ç”Ÿæˆå¤šç±»å‹æµ‹è¯•
        test_files = FilesDict()
        
        # å•å…ƒæµ‹è¯•
        unit_tests = await self.test_generator.generate_unit_tests(
            files, test_requirements['unit']
        )
        test_files.update(unit_tests)
        
        # é›†æˆæµ‹è¯•
        integration_tests = await self.test_generator.generate_integration_tests(
            files, test_requirements['integration']
        )
        test_files.update(integration_tests)
        
        # æ€§èƒ½æµ‹è¯•
        if test_requirements.get('performance'):
            perf_tests = await self.performance_tester.generate_performance_tests(
                files, test_requirements['performance']
            )
            test_files.update(perf_tests)
        
        # å®‰å…¨æµ‹è¯•
        if test_requirements.get('security'):
            security_tests = await self.security_tester.generate_security_tests(
                files, test_requirements['security']
            )
            test_files.update(security_tests)
        
        # ç³»ç»Ÿæµ‹è¯•
        system_tests = await self.test_generator.generate_system_tests(
            files, test_requirements.get('system', {})
        )
        test_files.update(system_tests)
        
        # ç”Ÿæˆæµ‹è¯•é…ç½®æ–‡ä»¶
        config_files = await self._generate_test_configurations(files, test_files)
        test_files.update(config_files)
        
        print(f"âœ… ç”Ÿæˆäº† {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
        return test_files
    
    async def execute_tests_async(self, files: FilesDict, test_files: FilesDict) -> TestResult:
        """æ‰§è¡Œæµ‹è¯• - å‡çº§ç‰ˆï¼ˆå¼‚æ­¥ï¼‰"""
        print("ğŸš€ æ‰§è¡Œå…¨é¢æµ‹è¯•å¥—ä»¶...")
        
        # å‡†å¤‡æµ‹è¯•ç¯å¢ƒ
        test_env = await self._prepare_test_environment(files, test_files)
        
        # æ‰§è¡Œä¸åŒç±»å‹çš„æµ‹è¯•
        test_results = {}
        
        # å•å…ƒæµ‹è¯•
        unit_result = await self._execute_unit_tests(test_env)
        test_results['unit'] = unit_result
        
        # é›†æˆæµ‹è¯•
        integration_result = await self._execute_integration_tests(test_env)
        test_results['integration'] = integration_result
        
        # æ€§èƒ½æµ‹è¯•
        if self._has_performance_tests(test_files):
            perf_result = await self.performance_tester.execute_performance_tests(test_env)
            test_results['performance'] = perf_result
        
        # å®‰å…¨æµ‹è¯•
        if self._has_security_tests(test_files):
            security_result = await self.security_tester.execute_security_tests(test_env)
            test_results['security'] = security_result
        
        # è¦†ç›–ç‡åˆ†æ
        coverage_analysis = await self.coverage_analyzer.analyze_detailed_coverage(
            test_env, test_results
        )
        
        # ç¼ºé™·åˆ†æ
        defect_analysis = await self.defect_analyzer.analyze_defects(
            test_results, coverage_analysis
        )
        
        # ç”Ÿæˆç»¼åˆæµ‹è¯•ç»“æœ
        overall_result = self._compile_test_results(
            test_results, coverage_analysis, defect_analysis
        )
        
        # å­¦ä¹ å’Œä¼˜åŒ–
        await self._learn_from_test_execution(
            test_results, coverage_analysis, overall_result
        )
        
        return overall_result
    
    async def analyze_results(self, test_result: TestResult) -> Dict[str, Any]:
        """æ·±åº¦ç»“æœåˆ†æ - å‡çº§ç‰ˆ"""
        analysis = {
            "summary": self._generate_test_summary(test_result),
            "quality_assessment": await self._assess_test_quality(test_result),
            "coverage_analysis": await self._detailed_coverage_analysis(test_result),
            "performance_analysis": await self._analyze_performance_results(test_result),
            "defect_patterns": await self.defect_analyzer.identify_patterns(test_result),
            "risk_assessment": await self._assess_testing_risks(test_result),
            "optimization_suggestions": await self.optimization_engine.generate_suggestions(test_result),
            "predictive_insights": await self._generate_predictive_insights(test_result),
            "actionable_recommendations": await self._generate_actionable_recommendations(test_result)
        }
        
        return analysis
    
    async def continuous_testing(self, files: FilesDict) -> AsyncIterator[TestResult]:
        """æŒç»­æµ‹è¯• - å‡çº§ç‰ˆ"""
        print("ğŸ”„ å¯åŠ¨æŒç»­æµ‹è¯•æ¨¡å¼...")
        
        while True:
            try:
                # æ£€æµ‹ä»£ç å˜æ›´
                changes = await self._detect_code_changes(files)
                
                if changes:
                    # æ™ºèƒ½æµ‹è¯•é€‰æ‹©
                    selected_tests = await self._select_relevant_tests(changes)
                    
                    # æ‰§è¡Œé€‰å®šçš„æµ‹è¯•
                    test_result = await self._execute_selected_tests(selected_tests)
                    
                    # å®æ—¶åˆ†æ
                    analysis = await self.analyze_results(test_result)
                    
                    yield test_result
                    
                    # å¦‚æœå‘ç°ä¸¥é‡é—®é¢˜ï¼Œè§¦å‘å…¨é¢æµ‹è¯•
                    if analysis['risk_assessment']['level'] == 'high':
                        full_tests = await self.generate_tests_async(files)
                        full_result = await self.execute_tests_async(files, full_tests)
                        yield full_result
                
                # ç­‰å¾…ä¸‹ä¸€è½®æ£€æŸ¥
                await asyncio.sleep(60)  # 1åˆ†é’Ÿæ£€æŸ¥é—´éš”
                
            except Exception as e:
                print(f"æŒç»­æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                await asyncio.sleep(300)  # é”™è¯¯åç­‰å¾…5åˆ†é’Ÿ
    
    async def _analyze_code_structure(self, files: FilesDict) -> Dict[str, Any]:
        """åˆ†æä»£ç ç»“æ„"""
        structure_prompt = """
åˆ†æä»¥ä¸‹ä»£ç çš„ç»“æ„ï¼Œè¯†åˆ«ï¼š
1. ä¸»è¦åŠŸèƒ½æ¨¡å—
2. ç±»å’Œæ–¹æ³•
3. ä¾èµ–å…³ç³»
4. å¤æ‚åº¦
5. æ½œåœ¨çš„æµ‹è¯•ç‚¹

ä»£ç æ–‡ä»¶ï¼š
"""
        
        for filename, content in files.items():
            structure_prompt += f"\n--- {filename} ---\n{content}\n"
        
        response = self.ai.start(
            system="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç ç»“æ„åˆ†æå¸ˆ",
            user=structure_prompt,
            step_name="code_structure_analysis"
        )
        
        # è§£æåˆ†æç»“æœ
        return {
            "modules": [],
            "classes": [],
            "functions": [],
            "dependencies": [],
            "complexity_score": 0.7,
            "test_points": []
        }
    
    async def _identify_test_requirements(self, files: FilesDict, requirements: Optional[str], 
                                        analysis: Dict[str, Any]) -> Dict[str, Any]:
        """è¯†åˆ«æµ‹è¯•éœ€æ±‚"""
        return {
            "unit": {"functions": analysis.get("functions", [])},
            "integration": {"modules": analysis.get("modules", [])},
            "performance": {"critical_paths": []},
            "security": {"security_points": []},
            "system": {"user_scenarios": []}
        }
    
    async def _prepare_test_environment(self, files: FilesDict, test_files: FilesDict) -> Dict[str, Any]:
        """å‡†å¤‡æµ‹è¯•ç¯å¢ƒ"""
        # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
        test_dir = self.work_dir / f"test_env_{uuid.uuid4().hex[:8]}"
        test_dir.mkdir(exist_ok=True)
        
        # å¤åˆ¶æºä»£ç æ–‡ä»¶
        for filename, content in files.items():
            file_path = test_dir / filename
            file_path.parent.mkdir(parents=True, exist_ok=True)
            file_path.write_text(content, encoding='utf-8')
        
        # å¤åˆ¶æµ‹è¯•æ–‡ä»¶
        for filename, content in test_files.items():
            file_path = test_dir / filename
            file_path.parent.mkdir(parents=True, exist_ok=True)
            file_path.write_text(content, encoding='utf-8')
        
        return {
            "test_dir": test_dir,
            "source_files": files,
            "test_files": test_files,
            "environment": os.environ.copy()
        }
    
    def _compile_test_results(self, test_results: Dict, coverage: Dict, defects: Dict) -> TestResult:
        """ç¼–è¯‘æµ‹è¯•ç»“æœ"""
        total_tests = sum(result.get('total', 0) for result in test_results.values())
        passed_tests = sum(result.get('passed', 0) for result in test_results.values())
        failed_tests = sum(result.get('failed', 0) for result in test_results.values())
        
        return TestResult(
            success=failed_tests == 0,
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            coverage_percentage=coverage.get('overall', 0.0),
            execution_time=sum(result.get('time', 0) for result in test_results.values()),
            test_details=test_results,
            coverage_details=coverage,
            performance_metrics=test_results.get('performance', {}),
            security_findings=test_results.get('security', {}),
            defect_analysis=defects
        )
    
    # å®ç°åŸºç±»çš„æŠ½è±¡æ–¹æ³•
    def generate_tests(self, files_dict: FilesDict, requirements: Dict[str, Any]) -> FilesDict:
        """ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ - åŸºç±»æ–¹æ³•å®ç°"""
        try:
            # è½¬æ¢å‚æ•°æ ¼å¼å¹¶è°ƒç”¨å¼‚æ­¥æ–¹æ³•
            import asyncio
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            # è½¬æ¢requirementsæ ¼å¼
            requirements_str = str(requirements) if requirements else None
            
            # è°ƒç”¨å¼‚æ­¥ç‰ˆæœ¬çš„æ–¹æ³•
            result = loop.run_until_complete(
                self.generate_tests_async(files_dict, requirements_str)
            )
            
            loop.close()
            return result
            
        except Exception as e:
            print(f"ç”Ÿæˆæµ‹è¯•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return FilesDict()
    
    def execute_tests(self, files_dict: FilesDict) -> TestResult:
        """æ‰§è¡Œæµ‹è¯• - åŸºç±»æ–¹æ³•å®ç°"""
        try:
            # ç”ŸæˆåŸºæœ¬æµ‹è¯•æ–‡ä»¶
            test_files = self.generate_tests(files_dict, {})
            
            # è½¬æ¢å‚æ•°æ ¼å¼å¹¶è°ƒç”¨å¼‚æ­¥æ–¹æ³•
            import asyncio
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            # è°ƒç”¨å¼‚æ­¥ç‰ˆæœ¬çš„æ–¹æ³•
            result = loop.run_until_complete(
                self.execute_tests_async(files_dict, test_files)
            )
            
            loop.close()
            return result
            
        except Exception as e:
            print(f"æ‰§è¡Œæµ‹è¯•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return TestResult(
                success=False,
                total_tests=0,
                passed_tests=0,
                failed_tests=1,
                coverage_percentage=0.0,
                execution_time=0.0,
                failures=[f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}"],
                errors=[]
            )
    
    def analyze_coverage(self, test_result: TestResult) -> Dict[str, float]:
        """åˆ†ææµ‹è¯•è¦†ç›–ç‡ - åŸºç±»æ–¹æ³•å®ç°"""
        try:
            # ä»æµ‹è¯•ç»“æœä¸­æå–è¦†ç›–ç‡ä¿¡æ¯
            coverage_details = getattr(test_result, 'coverage_details', {})
            
            return {
                "line_coverage": coverage_details.get("line_coverage", 0.0),
                "branch_coverage": coverage_details.get("branch_coverage", 0.0), 
                "function_coverage": coverage_details.get("function_coverage", 0.0),
                "statement_coverage": coverage_details.get("statement_coverage", 0.0),
                "overall_coverage": test_result.coverage_percentage or 0.0
            }
        except Exception as e:
            print(f"åˆ†æè¦†ç›–ç‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "line_coverage": 0.0,
                "branch_coverage": 0.0,
                "function_coverage": 0.0, 
                "statement_coverage": 0.0,
                "overall_coverage": 0.0
            }
    
    def diagnose_failures(self, test_result: TestResult, files_dict: FilesDict) -> List[str]:
        """è¯Šæ–­æµ‹è¯•å¤±è´¥åŸå› """
        diagnoses = []
        
        try:
            # åˆ†æå¤±è´¥çš„æµ‹è¯•
            if hasattr(test_result, 'failures') and test_result.failures:
                for failure in test_result.failures:
                    diagnosis = self._analyze_test_failure(failure, files_dict)
                    if diagnosis:
                        diagnoses.append(diagnosis)
            
            # åˆ†æé”™è¯¯
            if hasattr(test_result, 'errors') and test_result.errors:
                for error in test_result.errors:
                    diagnosis = self._analyze_test_error(error, files_dict)
                    if diagnosis:
                        diagnoses.append(diagnosis)
            
            # åˆ†æä½è¦†ç›–ç‡
            if test_result.coverage_percentage and test_result.coverage_percentage < 0.8:
                diagnoses.append(f"æµ‹è¯•è¦†ç›–ç‡è¿‡ä½ ({test_result.coverage_percentage:.1%})ï¼Œå»ºè®®å¢åŠ æµ‹è¯•ç”¨ä¾‹")
            
            # åˆ†ææ€§èƒ½é—®é¢˜
            if test_result.execution_time and test_result.execution_time > 60:
                diagnoses.append(f"æµ‹è¯•æ‰§è¡Œæ—¶é—´è¿‡é•¿ ({test_result.execution_time:.1f}ç§’)ï¼Œå¯èƒ½å­˜åœ¨æ€§èƒ½é—®é¢˜")
            
            # åˆ†æé€šè¿‡ç‡
            if test_result.total_tests > 0:
                pass_rate = test_result.passed_tests / test_result.total_tests
                if pass_rate < 0.9:
                    diagnoses.append(f"æµ‹è¯•é€šè¿‡ç‡è¾ƒä½ ({pass_rate:.1%})ï¼Œéœ€è¦æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")
            
            # å¦‚æœæ²¡æœ‰å…·ä½“è¯Šæ–­ï¼Œæä¾›ä¸€èˆ¬æ€§å»ºè®®
            if not diagnoses and test_result.failed_tests > 0:
                diagnoses.append("å­˜åœ¨æµ‹è¯•å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥é”™è¯¯æ—¥å¿—å’Œæµ‹è¯•ç”¨ä¾‹é€»è¾‘")
            
        except Exception as e:
            diagnoses.append(f"è¯Šæ–­æµ‹è¯•å¤±è´¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        
        return diagnoses
    
    def final_evaluation(self, deploy_result: DeployResult) -> float:
        """æœ€ç»ˆè¯„åˆ†"""
        try:
            score = 0.0
            
            # éƒ¨ç½²æˆåŠŸæ€§è¯„åˆ† (40%)
            if deploy_result.success:
                score += 0.4
            else:
                score += 0.1  # éƒ¨åˆ†åˆ†æ•°ï¼Œè‡³å°‘å°è¯•äº†éƒ¨ç½²
            
            # æ€§èƒ½è¯„åˆ† (25%)
            if hasattr(deploy_result, 'performance_metrics'):
                perf_metrics = deploy_result.performance_metrics
                if isinstance(perf_metrics, dict):
                    # å“åº”æ—¶é—´è¯„åˆ†
                    response_time = perf_metrics.get('response_time', 1.0)
                    if response_time < 0.5:
                        score += 0.15
                    elif response_time < 1.0:
                        score += 0.10
                    else:
                        score += 0.05
                    
                    # å†…å­˜ä½¿ç”¨è¯„åˆ†
                    memory_usage = perf_metrics.get('memory_usage', 100.0)
                    if memory_usage < 50.0:
                        score += 0.10
                    elif memory_usage < 100.0:
                        score += 0.05
            
            # å¯ç”¨æ€§è¯„åˆ† (20%)
            if hasattr(deploy_result, 'availability') and deploy_result.availability:
                if deploy_result.availability > 0.99:
                    score += 0.20
                elif deploy_result.availability > 0.95:
                    score += 0.15
                elif deploy_result.availability > 0.90:
                    score += 0.10
                else:
                    score += 0.05
            
            # å®‰å…¨æ€§è¯„åˆ† (15%)
            if hasattr(deploy_result, 'security_scan_result'):
                security_result = deploy_result.security_scan_result
                if isinstance(security_result, dict):
                    vulnerabilities = security_result.get('vulnerabilities', [])
                    if len(vulnerabilities) == 0:
                        score += 0.15
                    elif len(vulnerabilities) <= 2:
                        score += 0.10
                    else:
                        score += 0.05
            
            # ç¡®ä¿åˆ†æ•°åœ¨0-1èŒƒå›´å†…
            score = max(0.0, min(1.0, score))
            
            return score
            
        except Exception as e:
            print(f"æœ€ç»ˆè¯„åˆ†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return 0.5  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
    
    def _analyze_test_failure(self, failure: str, files_dict: FilesDict) -> str:
        """åˆ†æå…·ä½“çš„æµ‹è¯•å¤±è´¥"""
        try:
            # ç®€åŒ–çš„å¤±è´¥åˆ†æé€»è¾‘
            if "AssertionError" in failure:
                return "æ–­è¨€å¤±è´¥ï¼šé¢„æœŸç»“æœä¸å®é™…ç»“æœä¸åŒ¹é…ï¼Œæ£€æŸ¥æµ‹è¯•é€»è¾‘å’Œå®ç°"
            elif "AttributeError" in failure:
                return "å±æ€§é”™è¯¯ï¼šè®¿é—®äº†ä¸å­˜åœ¨çš„å±æ€§æˆ–æ–¹æ³•ï¼Œæ£€æŸ¥å¯¹è±¡æ¥å£"
            elif "TypeError" in failure:
                return "ç±»å‹é”™è¯¯ï¼šå‚æ•°ç±»å‹ä¸åŒ¹é…ï¼Œæ£€æŸ¥å‡½æ•°è°ƒç”¨å‚æ•°"
            elif "ValueError" in failure:
                return "å€¼é”™è¯¯ï¼šå‚æ•°å€¼ä¸åˆæ³•ï¼Œæ£€æŸ¥è¾“å…¥éªŒè¯é€»è¾‘"
            elif "ImportError" in failure:
                return "å¯¼å…¥é”™è¯¯ï¼šæ¨¡å—æˆ–åŒ…æ— æ³•å¯¼å…¥ï¼Œæ£€æŸ¥ä¾èµ–å’Œè·¯å¾„"
            else:
                return f"æµ‹è¯•å¤±è´¥ï¼š{failure[:100]}..."
        except:
            return "æ— æ³•åˆ†æçš„æµ‹è¯•å¤±è´¥"
    
    def _analyze_test_error(self, error: str, files_dict: FilesDict) -> str:
        """åˆ†æå…·ä½“çš„æµ‹è¯•é”™è¯¯"""
        try:
            # ç®€åŒ–çš„é”™è¯¯åˆ†æé€»è¾‘
            if "timeout" in error.lower():
                return "è¶…æ—¶é”™è¯¯ï¼šæµ‹è¯•æ‰§è¡Œè¶…æ—¶ï¼Œå¯èƒ½å­˜åœ¨æ­»å¾ªç¯æˆ–æ€§èƒ½é—®é¢˜"
            elif "connection" in error.lower():
                return "è¿æ¥é”™è¯¯ï¼šç½‘ç»œæˆ–æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œæ£€æŸ¥å¤–éƒ¨ä¾èµ–"
            elif "permission" in error.lower():
                return "æƒé™é”™è¯¯ï¼šæ–‡ä»¶æˆ–èµ„æºè®¿é—®æƒé™ä¸è¶³"
            elif "memory" in error.lower():
                return "å†…å­˜é”™è¯¯ï¼šå†…å­˜ä¸è¶³æˆ–å†…å­˜æ³„æ¼"
            else:
                return f"æµ‹è¯•é”™è¯¯ï¼š{error[:100]}..."
        except:
            return "æ— æ³•åˆ†æçš„æµ‹è¯•é”™è¯¯"


class IntelligentTestGenerator:
    """æ™ºèƒ½æµ‹è¯•ç”Ÿæˆå™¨"""
    
    def __init__(self, ai: AI):
        self.ai = ai
    
    async def generate_unit_tests(self, files: FilesDict, requirements: Dict) -> FilesDict:
        """ç”Ÿæˆæ™ºèƒ½å•å…ƒæµ‹è¯•"""
        test_files = FilesDict()
        
        for filename, content in files.items():
            if self._should_generate_unit_tests(filename, content):
                test_content = await self._generate_unit_test_for_file(filename, content)
                test_filename = self._get_test_filename(filename, "unit")
                test_files[test_filename] = test_content
        
        return test_files
    
    async def generate_integration_tests(self, files: FilesDict, requirements: Dict) -> FilesDict:
        """ç”Ÿæˆé›†æˆæµ‹è¯•"""
        return FilesDict()
    
    async def generate_system_tests(self, files: FilesDict, requirements: Dict) -> FilesDict:
        """ç”Ÿæˆç³»ç»Ÿæµ‹è¯•"""
        return FilesDict()
    
    def _should_generate_unit_tests(self, filename: str, content: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸ºæ–‡ä»¶ç”Ÿæˆå•å…ƒæµ‹è¯•"""
        return filename.endswith('.py') and 'def ' in content
    
    async def _generate_unit_test_for_file(self, filename: str, content: str) -> str:
        """ä¸ºå•ä¸ªæ–‡ä»¶ç”Ÿæˆå•å…ƒæµ‹è¯•"""
        prompt = f"""
ä¸ºä»¥ä¸‹Pythonæ–‡ä»¶ç”Ÿæˆå…¨é¢çš„å•å…ƒæµ‹è¯•ï¼š

æ–‡ä»¶å: {filename}
ä»£ç å†…å®¹:
{content}

è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„æµ‹è¯•ï¼š
1. æ‰€æœ‰å…¬å…±æ–¹æ³•çš„æµ‹è¯•
2. è¾¹ç•Œæ¡ä»¶æµ‹è¯•
3. å¼‚å¸¸æƒ…å†µæµ‹è¯•
4. Mockå¯¹è±¡ä½¿ç”¨
5. å‚æ•°åŒ–æµ‹è¯•

ä½¿ç”¨pytestæ¡†æ¶ã€‚
"""
        
        response = self.ai.start(
            system="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æµ‹è¯•å·¥ç¨‹å¸ˆï¼Œä¸“é—¨ç¼–å†™é«˜è´¨é‡çš„å•å…ƒæµ‹è¯•ã€‚",
            user=prompt,
            step_name="generate_unit_test"
        )
        
        return response[-1].content
    
    def _get_test_filename(self, source_filename: str, test_type: str) -> str:
        """ç”Ÿæˆæµ‹è¯•æ–‡ä»¶å"""
        base_name = Path(source_filename).stem
        return f"tests/test_{test_type}_{base_name}.py"


class CoverageAnalyzer:
    """è¦†ç›–ç‡åˆ†æå™¨"""
    
    def __init__(self, ai: AI):
        self.ai = ai
    
    async def analyze_detailed_coverage(self, test_env: Dict, test_results: Dict) -> Dict[str, Any]:
        """åˆ†æè¯¦ç»†æµ‹è¯•è¦†ç›–ç‡"""
        return {
            "line_coverage": 0.85,
            "branch_coverage": 0.80,
            "function_coverage": 0.90,
            "overall": 0.85,
            "uncovered_lines": [],
            "critical_gaps": []
        }


class PerformanceTester:
    """æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self, ai: AI):
        self.ai = ai
    
    async def generate_performance_tests(self, files: FilesDict, requirements: Dict) -> FilesDict:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•"""
        return FilesDict()
    
    async def execute_performance_tests(self, test_env: Dict) -> Dict[str, Any]:
        """æ‰§è¡Œæ€§èƒ½æµ‹è¯•"""
        return {
            "response_time": 0.1,
            "throughput": 1000,
            "memory_usage": 50.0,
            "cpu_usage": 30.0
        }


class SecurityTester:
    """å®‰å…¨æµ‹è¯•å™¨"""
    
    def __init__(self, ai: AI):
        self.ai = ai
    
    async def generate_security_tests(self, files: FilesDict, requirements: Dict) -> FilesDict:
        """ç”Ÿæˆå®‰å…¨æµ‹è¯•"""
        return FilesDict()
    
    async def execute_security_tests(self, test_env: Dict) -> Dict[str, Any]:
        """æ‰§è¡Œå®‰å…¨æµ‹è¯•"""
        return {
            "vulnerabilities": [],
            "security_score": 0.95,
            "recommendations": []
        }


class DefectAnalyzer:
    """ç¼ºé™·åˆ†æå™¨"""
    
    def __init__(self, ai: AI):
        self.ai = ai
    
    async def analyze_defects(self, test_results: Dict, coverage: Dict) -> Dict[str, Any]:
        """åˆ†æç¼ºé™·"""
        return {
            "defect_count": 0,
            "severity_distribution": {},
            "root_causes": [],
            "patterns": []
        }
    
    async def identify_patterns(self, test_result: TestResult) -> List[DefectPattern]:
        """è¯†åˆ«ç¼ºé™·æ¨¡å¼"""
        return []


class TestOptimizationEngine:
    """æµ‹è¯•ä¼˜åŒ–å¼•æ“"""
    
    def __init__(self, ai: AI):
        self.ai = ai
    
    async def generate_suggestions(self, test_result: TestResult) -> List[TestOptimization]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        return []