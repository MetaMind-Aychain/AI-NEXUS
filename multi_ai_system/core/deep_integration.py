"""
æ·±åº¦é›†æˆGPT-ENGINEERæ ¸å¿ƒæ¨¡å—

å°†å‡çº§ç‰ˆAIä¸åŸæœ‰GPT-ENGINEERæ¶æ„æ·±åº¦èåˆ
ç¡®ä¿å®Œå…¨å…¼å®¹æ€§å’Œæœ€ä¼˜æ€§èƒ½
"""

import json
import uuid
import asyncio
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from pathlib import Path

from gpt_engineer.core.ai import AI
from gpt_engineer.core.base_agent import BaseAgent
from gpt_engineer.core.base_execution_env import BaseExecutionEnv
from gpt_engineer.core.base_memory import BaseMemory
from gpt_engineer.core.default.simple_agent import SimpleAgent
from gpt_engineer.core.default.steps import (
    gen_code, gen_entrypoint, improve_fn, execute_entrypoint,
    setup_sys_prompt, setup_sys_prompt_existing_code
)
from gpt_engineer.core.files_dict import FilesDict
from gpt_engineer.core.preprompts_holder import PrepromptsHolder
from gpt_engineer.core.prompt import Prompt
from gpt_engineer.core.default.disk_memory import DiskMemory
from gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv

from .base_interfaces import (
    BaseSupervisorAI, BaseTestAI, BaseSharedMemory,
    DevPlan, DevelopmentEvent, TestResult, TaskStatus
)


class DeepIntegratedDevAI(SimpleAgent):
    """
    æ·±åº¦é›†æˆçš„å¼€å‘AI
    
    å®Œå…¨ç»§æ‰¿åŸæœ‰GPT-ENGINEERçš„SimpleAgentï¼ŒåŒæ—¶æ‰©å±•é«˜çº§åŠŸèƒ½
    ç¡®ä¿å‘åå…¼å®¹æ€§å’Œæ— ç¼é›†æˆ
    """
    
    def __init__(
        self,
        memory: BaseMemory,
        execution_env: BaseExecutionEnv,
        ai: AI = None,
        preprompts_holder: PrepromptsHolder = None,
        supervisor_ai: Optional[BaseSupervisorAI] = None,
        test_ai: Optional[BaseTestAI] = None,
        shared_memory: Optional[BaseSharedMemory] = None
    ):
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°ï¼Œç¡®ä¿å®Œå…¨å…¼å®¹
        super().__init__(memory, execution_env, ai, preprompts_holder)
        
        # æ‰©å±•åŠŸèƒ½ç»„ä»¶
        self.supervisor_ai = supervisor_ai
        self.test_ai = test_ai
        self.shared_memory = shared_memory
        
        # æ·±åº¦é›†æˆçŠ¶æ€
        self.integration_context = {
            "current_step": None,
            "step_history": [],
            "ai_feedback": [],
            "quality_metrics": [],
            "test_results": []
        }
        
        # æ€§èƒ½ä¼˜åŒ–
        self.use_optimized_prompts = True
        self.enable_smart_caching = True
        self.enable_incremental_updates = True
        
        print("ğŸ”— æ·±åº¦é›†æˆå¼€å‘AIå·²åˆå§‹åŒ–")
        print(f"   åŸºäºGPT-ENGINEER: {type(super()).__name__}")
        print(f"   ç›‘ç®¡AIé›†æˆ: {'âœ…' if supervisor_ai else 'âŒ'}")
        print(f"   æµ‹è¯•AIé›†æˆ: {'âœ…' if test_ai else 'âŒ'}")
        print(f"   å…±äº«è®°å¿†: {'âœ…' if shared_memory else 'âŒ'}")
    
    def init(self, prompt: str) -> FilesDict:
        """
        æ·±åº¦é›†æˆçš„é¡¹ç›®åˆå§‹åŒ–
        
        å®Œå…¨å…¼å®¹åŸæœ‰çš„initæ–¹æ³•ï¼ŒåŒæ—¶å¢å¼ºåŠŸèƒ½
        """
        print("ğŸš€ å¼€å§‹æ·±åº¦é›†æˆé¡¹ç›®åˆå§‹åŒ–...")
        
        # è®°å½•å½“å‰æ­¥éª¤
        self.integration_context["current_step"] = "init"
        self.integration_context["step_history"].append({
            "step": "init",
            "timestamp": datetime.now(),
            "prompt": prompt
        })
        
        # æ™ºèƒ½æç¤ºä¼˜åŒ–
        if self.use_optimized_prompts:
            enhanced_prompt = self._enhance_init_prompt(prompt)
        else:
            enhanced_prompt = prompt
        
        # è°ƒç”¨åŸå§‹çš„ç”Ÿæˆæ­¥éª¤ï¼Œç¡®ä¿å®Œå…¨å…¼å®¹
        messages = gen_code(
            ai=self.ai,
            prompt=Prompt(enhanced_prompt),
            memory=self.memory,
            preprompts=self.preprompts_holder.load()
        )
        
        # è·å–ç”Ÿæˆçš„æ–‡ä»¶
        files_dict = self.memory.to_dict()
        
        # ç”Ÿæˆå…¥å£ç‚¹
        entrypoint_messages = gen_entrypoint(
            ai=self.ai,
            files_dict=files_dict,
            memory=self.memory,
            preprompts=self.preprompts_holder.load()
        )
        
        # æ›´æ–°æ–‡ä»¶å­—å…¸
        updated_files_dict = self.memory.to_dict()
        
        # æ™ºèƒ½è´¨é‡æ£€æŸ¥
        if self.supervisor_ai:
            print("ğŸ‘ï¸ æ‰§è¡Œæ™ºèƒ½è´¨é‡æ£€æŸ¥...")
            quality_feedback = self._get_supervisor_feedback(updated_files_dict)
            self.integration_context["ai_feedback"].append(quality_feedback)
            
            # å¦‚æœè´¨é‡ä¸è¾¾æ ‡ï¼Œè¿›è¡Œä¼˜åŒ–
            if quality_feedback.get("needs_improvement", False):
                updated_files_dict = self._apply_quality_improvements(
                    updated_files_dict, quality_feedback
                )
        
        # æ™ºèƒ½æµ‹è¯•ç”Ÿæˆ
        if self.test_ai:
            print("ğŸ§ª æ‰§è¡Œæ™ºèƒ½æµ‹è¯•ç”Ÿæˆ...")
            test_feedback = self._generate_smart_tests(updated_files_dict, prompt)
            self.integration_context["test_results"].append(test_feedback)
        
        # ä¿å­˜åˆ°å…±äº«è®°å¿†
        if self.shared_memory:
            self._save_to_shared_memory("init_result", {
                "files": updated_files_dict,
                "prompt": prompt,
                "quality_feedback": self.integration_context["ai_feedback"],
                "test_results": self.integration_context["test_results"]
            })
        
        print(f"âœ… æ·±åº¦é›†æˆåˆå§‹åŒ–å®Œæˆï¼Œç”Ÿæˆ {len(updated_files_dict)} ä¸ªæ–‡ä»¶")
        
        return FilesDict(updated_files_dict)
    
    def improve(self, files_dict: FilesDict, user_feedback: str) -> FilesDict:
        """
        æ·±åº¦é›†æˆçš„ä»£ç æ”¹è¿›
        
        å®Œå…¨å…¼å®¹åŸæœ‰çš„improveæ–¹æ³•ï¼ŒåŒæ—¶å¢å¼ºåŠŸèƒ½
        """
        print("ğŸ”§ å¼€å§‹æ·±åº¦é›†æˆä»£ç æ”¹è¿›...")
        
        # è®°å½•å½“å‰æ­¥éª¤
        self.integration_context["current_step"] = "improve"
        self.integration_context["step_history"].append({
            "step": "improve",
            "timestamp": datetime.now(),
            "feedback": user_feedback
        })
        
        # å°†FilesDictè½¬æ¢ä¸ºå†…å­˜ä¸­çš„æ–‡ä»¶
        self.memory.update(files_dict)
        
        # æ™ºèƒ½åé¦ˆåˆ†æ
        if self.supervisor_ai:
            analyzed_feedback = self._analyze_feedback_with_supervisor(user_feedback, files_dict)
        else:
            analyzed_feedback = user_feedback
        
        # è°ƒç”¨åŸå§‹çš„æ”¹è¿›æ­¥éª¤ï¼Œç¡®ä¿å®Œå…¨å…¼å®¹
        messages = improve_fn(
            ai=self.ai,
            prompt=Prompt(analyzed_feedback),
            memory=self.memory,
            preprompts=self.preprompts_holder.load()
        )
        
        # è·å–æ”¹è¿›åçš„æ–‡ä»¶
        improved_files_dict = self.memory.to_dict()
        
        # æ™ºèƒ½è´¨é‡éªŒè¯
        if self.supervisor_ai:
            print("ğŸ‘ï¸ éªŒè¯æ”¹è¿›è´¨é‡...")
            quality_verification = self._verify_improvement_quality(
                files_dict, FilesDict(improved_files_dict), user_feedback
            )
            self.integration_context["ai_feedback"].append(quality_verification)
            
            # å¦‚æœæ”¹è¿›ä¸æ»¡æ„ï¼Œè¿›è¡Œè¿­ä»£ä¼˜åŒ–
            if quality_verification.get("needs_further_improvement", False):
                improved_files_dict = self._iterative_improvement(
                    improved_files_dict, quality_verification
                )
        
        # æ™ºèƒ½æµ‹è¯•æ›´æ–°
        if self.test_ai:
            print("ğŸ§ª æ›´æ–°æ™ºèƒ½æµ‹è¯•...")
            test_update = self._update_tests_for_changes(
                files_dict, FilesDict(improved_files_dict)
            )
            self.integration_context["test_results"].append(test_update)
        
        # ä¿å­˜åˆ°å…±äº«è®°å¿†
        if self.shared_memory:
            self._save_to_shared_memory("improve_result", {
                "original_files": files_dict,
                "improved_files": improved_files_dict,
                "feedback": user_feedback,
                "quality_verification": self.integration_context["ai_feedback"][-1] if self.integration_context["ai_feedback"] else None
            })
        
        print(f"âœ… æ·±åº¦é›†æˆæ”¹è¿›å®Œæˆ")
        
        return FilesDict(improved_files_dict)
    
    def execute_with_monitoring(self) -> Dict[str, Any]:
        """
        å¸¦ç›‘æ§çš„ä»£ç æ‰§è¡Œ
        
        é›†æˆåŸæœ‰çš„execute_entrypointåŠŸèƒ½ï¼Œå¢åŠ æ™ºèƒ½ç›‘æ§
        """
        print("ğŸƒ å¼€å§‹æ™ºèƒ½ç›‘æ§æ‰§è¡Œ...")
        
        # è®°å½•æ‰§è¡Œæ­¥éª¤
        self.integration_context["current_step"] = "execute"
        self.integration_context["step_history"].append({
            "step": "execute",
            "timestamp": datetime.now()
        })
        
        execution_result = {
            "success": False,
            "output": "",
            "error": None,
            "performance_metrics": {},
            "supervisor_analysis": {}
        }
        
        try:
            # è°ƒç”¨åŸå§‹çš„æ‰§è¡Œå‡½æ•°
            result = execute_entrypoint(
                ai=self.ai,
                execution_env=self.execution_env,
                memory=self.memory,
                preprompts=self.preprompts_holder.load()
            )
            
            execution_result["success"] = True
            execution_result["output"] = str(result) if result else ""
            
            # ç›‘ç®¡AIåˆ†ææ‰§è¡Œç»“æœ
            if self.supervisor_ai:
                supervisor_analysis = self._analyze_execution_with_supervisor(result)
                execution_result["supervisor_analysis"] = supervisor_analysis
            
            print("âœ… æ™ºèƒ½ç›‘æ§æ‰§è¡Œå®Œæˆ")
            
        except Exception as e:
            execution_result["error"] = str(e)
            print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
            
            # ç›‘ç®¡AIåˆ†æé”™è¯¯
            if self.supervisor_ai:
                error_analysis = self._analyze_error_with_supervisor(e)
                execution_result["supervisor_analysis"] = error_analysis
        
        return execution_result
    
    def _enhance_init_prompt(self, original_prompt: str) -> str:
        """æ™ºèƒ½æç¤ºå¢å¼º"""
        enhancement_prefix = """
è¯·æ ¹æ®ä»¥ä¸‹éœ€æ±‚ç”Ÿæˆé«˜è´¨é‡ã€å¯ç»´æŠ¤çš„ä»£ç ã€‚
é‡ç‚¹å…³æ³¨ï¼š
1. ä»£ç ç»“æ„æ¸…æ™°
2. é”™è¯¯å¤„ç†å®Œå–„
3. æ–‡æ¡£æ³¨é‡Šé½å…¨
4. éµå¾ªæœ€ä½³å®è·µ

ç”¨æˆ·éœ€æ±‚ï¼š
"""
        return enhancement_prefix + original_prompt
    
    def _get_supervisor_feedback(self, files_dict: Dict[str, str]) -> Dict[str, Any]:
        """è·å–ç›‘ç®¡AIåé¦ˆ"""
        try:
            # åˆ›å»ºä¸´æ—¶çš„ç›‘ç®¡ä¼šè¯
            dev_plan = DevPlan(
                tasks=["ä»£ç è´¨é‡æ£€æŸ¥"],
                estimated_time=1.0,
                dependencies={},
                milestones=["è´¨é‡éªŒè¯å®Œæˆ"]
            )
            
            supervision_id = asyncio.run(
                self.supervisor_ai.start_supervision(dev_plan)
            )
            
            quality_report = asyncio.run(
                self.supervisor_ai.analyze_quality(supervision_id, FilesDict(files_dict))
            )
            
            return {
                "supervision_id": supervision_id,
                "quality_score": quality_report.overall_score,
                "needs_improvement": quality_report.overall_score < 0.8,
                "suggestions": quality_report.suggestions,
                "detailed_analysis": quality_report.detailed_analysis
            }
            
        except Exception as e:
            print(f"ç›‘ç®¡AIåé¦ˆè·å–å¤±è´¥: {e}")
            return {"error": str(e), "needs_improvement": False}
    
    def _generate_smart_tests(self, files_dict: Dict[str, str], requirements: str) -> Dict[str, Any]:
        """ç”Ÿæˆæ™ºèƒ½æµ‹è¯•"""
        try:
            test_files = asyncio.run(
                self.test_ai.generate_tests(FilesDict(files_dict), requirements)
            )
            
            # å°†æµ‹è¯•æ–‡ä»¶æ·»åŠ åˆ°å†…å­˜ä¸­
            for test_filename, test_content in test_files.items():
                self.memory[test_filename] = test_content
            
            return {
                "test_files": test_files,
                "test_count": len(test_files),
                "success": True
            }
            
        except Exception as e:
            print(f"æ™ºèƒ½æµ‹è¯•ç”Ÿæˆå¤±è´¥: {e}")
            return {"error": str(e), "success": False}
    
    def _apply_quality_improvements(self, files_dict: Dict[str, str], 
                                  quality_feedback: Dict[str, Any]) -> Dict[str, str]:
        """åº”ç”¨è´¨é‡æ”¹è¿›"""
        if not quality_feedback.get("suggestions"):
            return files_dict
        
        # åŸºäºç›‘ç®¡AIçš„å»ºè®®è¿›è¡Œæ”¹è¿›
        improvement_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹è´¨é‡åˆ†æå»ºè®®æ”¹è¿›ä»£ç ï¼š

{json.dumps(quality_feedback.get('suggestions', []), ensure_ascii=False, indent=2)}

å½“å‰ä»£ç è´¨é‡è¯„åˆ†: {quality_feedback.get('quality_score', 'N/A')}
"""
        
        try:
            # ä½¿ç”¨improveåŠŸèƒ½è¿›è¡Œè´¨é‡æ”¹è¿›
            improved_files = self.improve(FilesDict(files_dict), improvement_prompt)
            return improved_files
        except Exception as e:
            print(f"è´¨é‡æ”¹è¿›å¤±è´¥: {e}")
            return files_dict
    
    def _analyze_feedback_with_supervisor(self, feedback: str, files_dict: FilesDict) -> str:
        """ä½¿ç”¨ç›‘ç®¡AIåˆ†æç”¨æˆ·åé¦ˆ"""
        try:
            # æ„å»ºåˆ†ææç¤º
            analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·åé¦ˆï¼Œå¹¶æä¾›è¯¦ç»†çš„æ”¹è¿›æŒ‡å¯¼ï¼š

ç”¨æˆ·åé¦ˆ: {feedback}

è¯·æä¾›ï¼š
1. åé¦ˆçš„æ ¸å¿ƒè¦æ±‚
2. å…·ä½“çš„æ”¹è¿›æ–¹å‘
3. å®ç°å»ºè®®
4. æ½œåœ¨é£é™©è¯„ä¼°
"""
            
            response = self.ai.start(
                system="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åˆ†æå¸ˆï¼Œæ“…é•¿ç†è§£ç”¨æˆ·éœ€æ±‚å¹¶æä¾›ç²¾ç¡®çš„æ”¹è¿›æŒ‡å¯¼ã€‚",
                user=analysis_prompt,
                step_name="feedback_analysis"
            )
            
            return response[-1].content
            
        except Exception as e:
            print(f"åé¦ˆåˆ†æå¤±è´¥: {e}")
            return feedback
    
    def _verify_improvement_quality(self, original_files: FilesDict, 
                                  improved_files: FilesDict, feedback: str) -> Dict[str, Any]:
        """éªŒè¯æ”¹è¿›è´¨é‡"""
        try:
            # æ¯”è¾ƒæ”¹è¿›å‰åçš„è´¨é‡
            original_quality = asyncio.run(
                self.supervisor_ai.analyze_quality("temp", original_files)
            )
            improved_quality = asyncio.run(
                self.supervisor_ai.analyze_quality("temp", improved_files)
            )
            
            improvement_score = improved_quality.overall_score - original_quality.overall_score
            
            return {
                "original_score": original_quality.overall_score,
                "improved_score": improved_quality.overall_score,
                "improvement": improvement_score,
                "needs_further_improvement": improvement_score < 0.1,
                "feedback_addressed": improvement_score > 0
            }
            
        except Exception as e:
            print(f"è´¨é‡éªŒè¯å¤±è´¥: {e}")
            return {"error": str(e), "needs_further_improvement": False}
    
    def _iterative_improvement(self, files_dict: Dict[str, str], 
                             quality_verification: Dict[str, Any]) -> Dict[str, str]:
        """è¿­ä»£æ”¹è¿›"""
        max_iterations = 3
        current_files = files_dict.copy()
        
        for i in range(max_iterations):
            if not quality_verification.get("needs_further_improvement", False):
                break
            
            print(f"ğŸ”„ æ‰§è¡Œè¿­ä»£æ”¹è¿› {i+1}/{max_iterations}")
            
            # æ„å»ºè¿­ä»£æ”¹è¿›æç¤º
            iteration_prompt = f"""
ä¸Šä¸€æ¬¡æ”¹è¿›çš„æ•ˆæœä¸å¤Ÿç†æƒ³ï¼Œè¯·è¿›ä¸€æ­¥ä¼˜åŒ–ï¼š

å½“å‰è´¨é‡è¯„åˆ†: {quality_verification.get('improved_score', 'N/A')}
ç›®æ ‡è´¨é‡è¯„åˆ†: 0.85+

è¯·é‡ç‚¹å…³æ³¨ï¼š
1. ä»£ç ç»“æ„ä¼˜åŒ–
2. æ€§èƒ½æå‡
3. é”™è¯¯å¤„ç†
4. ä»£ç è§„èŒƒ
"""
            
            try:
                improved_files = self.improve(FilesDict(current_files), iteration_prompt)
                current_files = improved_files
                
                # é‡æ–°éªŒè¯è´¨é‡
                quality_verification = self._verify_improvement_quality(
                    FilesDict(files_dict), FilesDict(current_files), iteration_prompt
                )
                
            except Exception as e:
                print(f"è¿­ä»£æ”¹è¿›å¤±è´¥: {e}")
                break
        
        return current_files
    
    def _update_tests_for_changes(self, original_files: FilesDict, 
                                improved_files: FilesDict) -> Dict[str, Any]:
        """ä¸ºä»£ç å˜æ›´æ›´æ–°æµ‹è¯•"""
        try:
            # æ£€æµ‹ä»£ç å˜æ›´
            changes_detected = len(improved_files) != len(original_files) or \
                             any(improved_files.get(k) != original_files.get(k) 
                                for k in improved_files.keys())
            
            if changes_detected:
                # ç”Ÿæˆæ–°çš„æµ‹è¯•
                updated_tests = asyncio.run(
                    self.test_ai.generate_tests(improved_files, "æ›´æ–°æµ‹è¯•ä»¥åæ˜ ä»£ç å˜æ›´")
                )
                
                return {
                    "changes_detected": True,
                    "updated_tests": updated_tests,
                    "test_count": len(updated_tests)
                }
            else:
                return {
                    "changes_detected": False,
                    "message": "æœªæ£€æµ‹åˆ°ä»£ç å˜æ›´"
                }
                
        except Exception as e:
            print(f"æµ‹è¯•æ›´æ–°å¤±è´¥: {e}")
            return {"error": str(e), "changes_detected": False}
    
    def _analyze_execution_with_supervisor(self, execution_result: Any) -> Dict[str, Any]:
        """ä½¿ç”¨ç›‘ç®¡AIåˆ†ææ‰§è¡Œç»“æœ"""
        try:
            analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹ä»£ç æ‰§è¡Œç»“æœï¼š

æ‰§è¡Œç»“æœ: {str(execution_result)}

è¯·æä¾›ï¼š
1. æ‰§è¡ŒçŠ¶æ€è¯„ä¼°
2. æ€§èƒ½åˆ†æ
3. æ½œåœ¨é—®é¢˜è¯†åˆ«
4. ä¼˜åŒ–å»ºè®®
"""
            
            response = self.ai.start(
                system="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç æ‰§è¡Œåˆ†æå¸ˆï¼Œæ“…é•¿è¯„ä¼°ä»£ç è¿è¡ŒçŠ¶æ€å’Œæ€§èƒ½ã€‚",
                user=analysis_prompt,
                step_name="execution_analysis"
            )
            
            return {
                "analysis": response[-1].content,
                "status": "success"
            }
            
        except Exception as e:
            return {"error": str(e), "status": "failed"}
    
    def _analyze_error_with_supervisor(self, error: Exception) -> Dict[str, Any]:
        """ä½¿ç”¨ç›‘ç®¡AIåˆ†æé”™è¯¯"""
        try:
            error_prompt = f"""
è¯·åˆ†æä»¥ä¸‹ä»£ç æ‰§è¡Œé”™è¯¯ï¼š

é”™è¯¯ç±»å‹: {type(error).__name__}
é”™è¯¯ä¿¡æ¯: {str(error)}

è¯·æä¾›ï¼š
1. é”™è¯¯åŸå› åˆ†æ
2. è§£å†³æ–¹æ¡ˆå»ºè®®
3. é¢„é˜²æªæ–½
4. ä»£ç ä¿®å¤æŒ‡å¯¼
"""
            
            response = self.ai.start(
                system="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é”™è¯¯è¯Šæ–­ä¸“å®¶ï¼Œæ“…é•¿åˆ†æå’Œè§£å†³ä»£ç é—®é¢˜ã€‚",
                user=error_prompt,
                step_name="error_analysis"
            )
            
            return {
                "error_analysis": response[-1].content,
                "error_type": type(error).__name__,
                "status": "analyzed"
            }
            
        except Exception as e:
            return {"error": str(e), "status": "analysis_failed"}
    
    def _save_to_shared_memory(self, key: str, data: Any):
        """ä¿å­˜åˆ°å…±äº«è®°å¿†"""
        if self.shared_memory:
            try:
                asyncio.run(
                    self.shared_memory.store_memory(key, data)
                )
            except Exception as e:
                print(f"å…±äº«è®°å¿†ä¿å­˜å¤±è´¥: {e}")
    
    async def develop_project_from_document(self, project_id: str, document_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¹æ®æ–‡æ¡£AIç”Ÿæˆçš„å¼€å‘æ–‡æ¡£è¿›è¡Œé¡¹ç›®å¼€å‘
        
        è¿™æ˜¯è¿æ¥æ–‡æ¡£AIå’Œå¼€å‘AIçš„æ ¸å¿ƒæ–¹æ³•
        """
        print(f"ğŸ› ï¸ å¼€å‘AIå¼€å§‹æ ¹æ®æ–‡æ¡£å¼€å‘é¡¹ç›®: {project_id}")
        
        try:
            # 1. è§£ææ–‡æ¡£å†…å®¹
            project_name = document_result.get('project_name', f'Project_{project_id}')
            features = document_result.get('features', [])
            tech_stack = document_result.get('tech_stack', [])
            architecture = document_result.get('architecture', {})
            requirements = document_result.get('detailed_requirements', '')
            
            print(f"ğŸ“‹ é¡¹ç›®ä¿¡æ¯: {project_name}")
            print(f"ğŸ”§ æŠ€æœ¯æ ˆ: {', '.join(tech_stack)}")
            print(f"âš¡ åŠŸèƒ½æ¨¡å—: {len(features)} ä¸ª")
            
            # 2. æ„å»ºå¼€å‘æç¤º
            development_prompt = self._build_development_prompt(
                project_name, features, tech_stack, architecture, requirements
            )
            
            # 3. å¯åŠ¨ç›‘ç£AIç›‘æ§
            if self.supervisor_ai:
                await self._start_development_supervision(project_id, document_result)
            
            # 4. ä½¿ç”¨GPT-Engineerçš„æ ¸å¿ƒåŠŸèƒ½è¿›è¡Œä»£ç ç”Ÿæˆ
            print("ğŸ’» è°ƒç”¨GPT-ENGINEERæ ¸å¿ƒå¼•æ“ç”Ÿæˆä»£ç ...")
            files_dict = self.init(development_prompt)
            
            # 5. ç›‘ç£AIéªŒè¯ç”Ÿæˆçš„ä»£ç 
            if self.supervisor_ai:
                validation_result = await self._validate_with_supervisor(project_id, files_dict)
                if not validation_result.get('approved', True):
                    print("ğŸ”„ ç›‘ç£AIè¦æ±‚æ”¹è¿›ï¼Œè¿›è¡Œè¿­ä»£å¼€å‘...")
                    improvement_feedback = validation_result.get('feedback', '')
                    files_dict = self.improve(files_dict, improvement_feedback)
            
            # 6. ç”Ÿæˆé¡¹ç›®ç»“æ„
            project_path = await self._setup_project_structure(project_id, files_dict)
            
            # 7. ç”Ÿæˆé…ç½®æ–‡ä»¶
            config_files = await self._generate_config_files(tech_stack, architecture)
            files_dict.update(config_files)
            
            # 8. ä¿å­˜æ‰€æœ‰æ–‡ä»¶åˆ°é¡¹ç›®ç›®å½•
            await self._save_project_files(project_path, files_dict)
            
            # 9. è®°å½•å¼€å‘ç»“æœ
            development_result = {
                "project_id": project_id,
                "project_name": project_name,
                "project_path": str(project_path),
                "files": dict(files_dict),
                "tech_stack": tech_stack,
                "features_implemented": features,
                "development_status": "completed",
                "timestamp": datetime.now().isoformat()
            }
            
            # 10. ä¿å­˜åˆ°å…±äº«è®°å¿†
            if self.shared_memory:
                await self.shared_memory.store_project_context(project_id, {
                    "development_result": development_result
                })
            
            print(f"âœ… é¡¹ç›®å¼€å‘å®Œæˆ: {project_path}")
            return development_result
            
        except Exception as e:
            print(f"âŒ é¡¹ç›®å¼€å‘å¤±è´¥: {e}")
            error_result = {
                "project_id": project_id,
                "error": str(e),
                "development_status": "failed",
                "timestamp": datetime.now().isoformat()
            }
            return error_result
    
    def _build_development_prompt(self, project_name: str, features: List[str], 
                                tech_stack: List[str], architecture: Dict, 
                                requirements: str) -> str:
        """æ„å»ºå¼€å‘æç¤º"""
        prompt_parts = [
            f"è¯·å¼€å‘ä¸€ä¸ªåä¸º'{project_name}'çš„é¡¹ç›®ã€‚",
            f"\næŠ€æœ¯è¦æ±‚:",
            f"- ä½¿ç”¨æŠ€æœ¯æ ˆ: {', '.join(tech_stack)}",
            f"- æ¶æ„æ¨¡å¼: {architecture.get('pattern', 'æ ‡å‡†æ¶æ„')}",
            f"\nåŠŸèƒ½éœ€æ±‚:",
        ]
        
        for i, feature in enumerate(features, 1):
            prompt_parts.append(f"{i}. {feature}")
        
        prompt_parts.extend([
            f"\nè¯¦ç»†è¦æ±‚:",
            requirements,
            f"\nè¯·ç”Ÿæˆå®Œæ•´çš„é¡¹ç›®ä»£ç ï¼ŒåŒ…æ‹¬:",
            f"- ä¸»è¦åŠŸèƒ½æ¨¡å—",
            f"- é…ç½®æ–‡ä»¶",
            f"- æ–‡æ¡£è¯´æ˜",
            f"- æµ‹è¯•ä»£ç ",
            f"\nç¡®ä¿ä»£ç è´¨é‡é«˜ï¼Œç»“æ„æ¸…æ™°ï¼Œéµå¾ªæœ€ä½³å®è·µã€‚"
        ])
        
        return "\n".join(prompt_parts)
    
    async def _start_development_supervision(self, project_id: str, document_result: Dict):
        """å¯åŠ¨å¼€å‘ç›‘ç£"""
        if hasattr(self.supervisor_ai, 'start_monitoring'):
            await self.supervisor_ai.start_monitoring(project_id, document_result)
    
    async def _validate_with_supervisor(self, project_id: str, files_dict: FilesDict) -> Dict:
        """ç›‘ç£AIéªŒè¯ä»£ç """
        if hasattr(self.supervisor_ai, 'validate_development'):
            return await self.supervisor_ai.validate_development(project_id, dict(files_dict))
        return {"approved": True}
    
    async def _setup_project_structure(self, project_id: str, files_dict: FilesDict) -> Path:
        """è®¾ç½®é¡¹ç›®ç»“æ„"""
        project_path = Path(f"real_ai_projects/{project_id}")
        project_path.mkdir(parents=True, exist_ok=True)
        return project_path
    
    async def _generate_config_files(self, tech_stack: List[str], architecture: Dict) -> Dict[str, str]:
        """ç”Ÿæˆé…ç½®æ–‡ä»¶"""
        config_files = {}
        
        # æ ¹æ®æŠ€æœ¯æ ˆç”Ÿæˆé…ç½®
        if 'python' in tech_stack:
            config_files['requirements.txt'] = self._generate_python_requirements()
        
        if 'docker' in tech_stack:
            config_files['Dockerfile'] = self._generate_dockerfile()
            config_files['docker-compose.yml'] = self._generate_docker_compose()
        
        if 'javascript' in tech_stack or 'react' in tech_stack:
            config_files['package.json'] = self._generate_package_json()
        
        return config_files
    
    async def _save_project_files(self, project_path: Path, files_dict: FilesDict):
        """ä¿å­˜é¡¹ç›®æ–‡ä»¶"""
        for filename, content in files_dict.items():
            file_path = project_path / filename
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            try:
                file_path.write_text(content, encoding='utf-8')
            except Exception as e:
                print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ {filename}: {e}")
    
    def _generate_python_requirements(self) -> str:
        """ç”ŸæˆPythonä¾èµ–"""
        return """fastapi==0.104.1
uvicorn[standard]==0.24.0
sqlalchemy==2.0.23
pydantic==2.5.0
python-dotenv==1.0.0
pytest==7.4.3
"""
    
    def _generate_dockerfile(self) -> str:
        """ç”ŸæˆDockerfile"""
        return """FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
"""
    
    def _generate_docker_compose(self) -> str:
        """ç”Ÿæˆdocker-compose.yml"""
        return """version: '3.8'

services:
  web:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=sqlite:///./app.db
    volumes:
      - ./:/app
"""
    
    def _generate_package_json(self) -> str:
        """ç”Ÿæˆpackage.json"""
        return """{
  "name": "ai-generated-project",
  "version": "1.0.0",
  "description": "AI generated project",
  "main": "index.js",
  "scripts": {
    "start": "node index.js",
    "dev": "nodemon index.js",
    "test": "jest"
  },
  "dependencies": {
    "express": "^4.18.2",
    "dotenv": "^16.0.3"
  },
  "devDependencies": {
    "nodemon": "^3.0.1",
    "jest": "^29.5.0"
  }
}
"""

    def get_integration_status(self) -> Dict[str, Any]:
        """è·å–é›†æˆçŠ¶æ€"""
        return {
            "integration_context": self.integration_context,
            "components_status": {
                "supervisor_ai": self.supervisor_ai is not None,
                "test_ai": self.test_ai is not None,
                "shared_memory": self.shared_memory is not None
            },
            "optimization_settings": {
                "optimized_prompts": self.use_optimized_prompts,
                "smart_caching": self.enable_smart_caching,
                "incremental_updates": self.enable_incremental_updates
            },
            "step_history": self.integration_context["step_history"],
            "ai_feedback_count": len(self.integration_context["ai_feedback"]),
            "test_results_count": len(self.integration_context["test_results"])
        }


class DeepIntegrationManager:
    """
    æ·±åº¦é›†æˆç®¡ç†å™¨
    
    ç»Ÿä¸€ç®¡ç†åŸæœ‰GPT-ENGINEERç»„ä»¶ä¸å‡çº§ç‰ˆAIçš„æ·±åº¦é›†æˆ
    """
    
    def __init__(self, work_dir: str = "./deep_integration_workspace"):
        self.work_dir = Path(work_dir)
        self.work_dir.mkdir(exist_ok=True)
        
        # GPT-ENGINEERæ ¸å¿ƒç»„ä»¶
        self.ai = None
        self.memory = None
        self.execution_env = None
        self.preprompts_holder = None
        
        # å‡çº§ç‰ˆAIç»„ä»¶
        self.supervisor_ai = None
        self.test_ai = None
        self.shared_memory = None
        
        # æ·±åº¦é›†æˆçš„å¼€å‘AI
        self.integrated_dev_ai = None
        
        print("ğŸ”— æ·±åº¦é›†æˆç®¡ç†å™¨å·²åˆå§‹åŒ–")
    
    def setup_gpt_engineer_core(self, ai: AI, memory_dir: Optional[str] = None, 
                               preprompts_path: Optional[str] = None):
        """è®¾ç½®GPT-ENGINEERæ ¸å¿ƒç»„ä»¶"""
        self.ai = ai
        
        # è®¾ç½®å†…å­˜
        if memory_dir:
            self.memory = DiskMemory(memory_dir)
        else:
            self.memory = DiskMemory(str(self.work_dir / "memory"))
        
        # è®¾ç½®æ‰§è¡Œç¯å¢ƒ
        self.execution_env = DiskExecutionEnv()
        
        # è®¾ç½®é¢„æç¤º
        if preprompts_path:
            self.preprompts_holder = PrepromptsHolder(Path(preprompts_path))
        else:
            # ä½¿ç”¨é»˜è®¤è·¯å¾„
            from gpt_engineer.core.default.paths import PREPROMPTS_PATH
            self.preprompts_holder = PrepromptsHolder(PREPROMPTS_PATH)
        
        print("âœ… GPT-ENGINEERæ ¸å¿ƒç»„ä»¶è®¾ç½®å®Œæˆ")
    
    def setup_upgraded_ai_components(self, supervisor_ai=None, test_ai=None, shared_memory=None):
        """è®¾ç½®å‡çº§ç‰ˆAIç»„ä»¶"""
        self.supervisor_ai = supervisor_ai
        self.test_ai = test_ai
        self.shared_memory = shared_memory
        
        print("âœ… å‡çº§ç‰ˆAIç»„ä»¶è®¾ç½®å®Œæˆ")
        print(f"   ç›‘ç®¡AI: {'âœ…' if supervisor_ai else 'âŒ'}")
        print(f"   æµ‹è¯•AI: {'âœ…' if test_ai else 'âŒ'}")
        print(f"   å…±äº«è®°å¿†: {'âœ…' if shared_memory else 'âŒ'}")
    
    def create_deep_integrated_agent(self) -> DeepIntegratedDevAI:
        """åˆ›å»ºæ·±åº¦é›†æˆçš„å¼€å‘AIä»£ç†"""
        if not self.ai or not self.memory or not self.execution_env:
            raise ValueError("è¯·å…ˆè®¾ç½®GPT-ENGINEERæ ¸å¿ƒç»„ä»¶")
        
        self.integrated_dev_ai = DeepIntegratedDevAI(
            memory=self.memory,
            execution_env=self.execution_env,
            ai=self.ai,
            preprompts_holder=self.preprompts_holder,
            supervisor_ai=self.supervisor_ai,
            test_ai=self.test_ai,
            shared_memory=self.shared_memory
        )
        
        print("ğŸš€ æ·±åº¦é›†æˆå¼€å‘AIä»£ç†åˆ›å»ºå®Œæˆ")
        return self.integrated_dev_ai
    
    def get_integration_summary(self) -> Dict[str, Any]:
        """è·å–é›†æˆæ‘˜è¦"""
        return {
            "gpt_engineer_core": {
                "ai_ready": self.ai is not None,
                "memory_ready": self.memory is not None,
                "execution_env_ready": self.execution_env is not None,
                "preprompts_ready": self.preprompts_holder is not None
            },
            "upgraded_components": {
                "supervisor_ai": self.supervisor_ai is not None,
                "test_ai": self.test_ai is not None,
                "shared_memory": self.shared_memory is not None
            },
            "integrated_agent": {
                "created": self.integrated_dev_ai is not None,
                "status": self.integrated_dev_ai.get_integration_status() if self.integrated_dev_ai else None
            },
            "work_directory": str(self.work_dir)
        }